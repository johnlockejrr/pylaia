#!/usr/bin/env python
from __future__ import absolute_import

import argparse
import os
from functools import partial

from torch.optim import RMSprop

import laia.logging as log
from laia.data import ImageDataLoader, TextImageFromTextTableDataset, FixedSizeSampler
from laia.engine import Trainer, Evaluator, HtrEngineWrapper
from laia.engine.engine import EPOCH_END, EPOCH_START
from laia.engine.feeders import ImageFeeder, ItemFeeder
from laia.hooks import Hook, HookCollection, action, Action, ActionCollection
from laia.hooks.conditions import Lowest, MultipleOf, GEqThan, ConsecutiveNonDecreasing
from laia.plugins.arguments import add_argument, args, add_defaults
from laia.plugins.arguments_types import NumberInOpenRange
from laia.plugins.loader import (
    TrainerLoader,
    ModelLoader,
    TrainerCheckpointLoader,
    ModelCheckpointLoader,
)
from laia.plugins.saver import (
    TrainerSaver,
    CheckpointSaver,
    RollingSaver,
    ModelCheckpointSaver,
    TrainerCheckpointSaver,
)
from laia.utils import SymbolsTable, ImageToTensor, TextToTensor

if __name__ == "__main__":
    add_defaults(
        "batch_size",
        "learning_rate",
        "momentum",
        "gpu",
        "max_epochs",
        "show_progress_bar",
        "train_path",
        "train_samples_per_epoch",
        "valid_samples_per_epoch",
        "iterations_per_update",
        "save_checkpoint_interval",
        "num_rolling_checkpoints",
    )
    add_argument(
        "syms",
        type=argparse.FileType("r"),
        help="Symbols table mapping from strings to integers",
    )
    add_argument(
        "img_dirs", type=str, nargs="+", help="Directory containing word images"
    )
    add_argument(
        "tr_txt_table",
        type=argparse.FileType("r"),
        help="Character transcriptions of each training image",
    )
    add_argument(
        "va_txt_table",
        type=argparse.FileType("r"),
        help="Character transcriptions of each validation image",
    )
    add_argument(
        "--delimiters",
        type=str,
        nargs="+",
        default=["<space>"],
        help="Sequence of characters representing the word delimiters",
    )
    add_argument(
        "--max_nondecreasing_epochs",
        type=NumberInOpenRange(int, vmin=0),
        help="Stop the training once there has been this number "
        "consecutive epochs without a new lowest validation CER",
    )
    add_argument(
        "--model_filename", type=str, default="model", help="File name of the model"
    )
    add_argument(
        "--checkpoint",
        type=str,
        default="ckpt.lowest-valid-cer*",
        help="Suffix of the checkpoint to use, can be a glob pattern",
    )
    args = args()

    syms = SymbolsTable(args.syms)

    model = ModelLoader(
        args.train_path, filename=args.model_filename, gpu=args.gpu
    ).load()
    if model is None:
        log.error('Could not find the model. Have you run "pylaia-htr-create-model"?')
        exit(1)
    model = model.cuda(args.gpu - 1) if args.gpu else model.cpu()
    log.info(
        "Model has {} parameters",
        sum(param.data.numel() for param in model.parameters()),
    )

    trainer = TrainerLoader(args.train_path, gpu=args.gpu).load()
    if trainer is None:
        optimizer = RMSprop(
            model.parameters(), lr=args.learning_rate, momentum=args.momentum
        )
        parameters = {
            "model": model,
            "criterion": None,  # Set automatically by HtrEngineWrapper
            "optimizer": optimizer,
            "batch_target_fn": ItemFeeder("txt"),
            "batch_id_fn": ItemFeeder("id"),  # Print image ids on exception
        }
        trainer = Trainer(**parameters)
        TrainerSaver(args.train_path).save(Trainer, **parameters)

    tr_ds = TextImageFromTextTableDataset(
        args.tr_txt_table,
        args.img_dirs,
        img_transform=ImageToTensor(),
        txt_transform=TextToTensor(syms),
    )

    tr_ds_loader = ImageDataLoader(
        dataset=tr_ds,
        image_channels=1,
        batch_size=args.batch_size,
        num_workers=8,
        shuffle=not bool(args.train_samples_per_epoch),
        sampler=FixedSizeSampler(tr_ds, args.train_samples_per_epoch)
        if args.train_samples_per_epoch
        else None,
    )

    # Set all these separately because they might change between executions
    trainer.iterations_per_update = args.iterations_per_update
    trainer.set_progress_bar("Train" if args.show_progress_bar else None)
    trainer.set_data_loader(tr_ds_loader)
    trainer.set_batch_input_fn(
        ImageFeeder(
            device=args.gpu,
            # keep_padded_tensors=False,
            parent_feeder=ItemFeeder("img"),
        )
    )

    va_ds = TextImageFromTextTableDataset(
        args.va_txt_table,
        args.img_dirs,
        img_transform=ImageToTensor(),
        txt_transform=TextToTensor(syms),
    )
    va_ds_loader = ImageDataLoader(
        dataset=va_ds,
        image_channels=1,
        batch_size=args.batch_size,
        num_workers=8,
        sampler=FixedSizeSampler(va_ds, args.valid_samples_per_epoch)
        if args.valid_samples_per_epoch
        else None,
    )

    evaluator = Evaluator(
        model=model,
        data_loader=va_ds_loader,
        batch_input_fn=ImageFeeder(device=args.gpu, parent_feeder=ItemFeeder("img")),
        batch_target_fn=ItemFeeder("txt"),
        batch_id_fn=ItemFeeder("id"),
    )
    evaluator.set_progress_bar("Valid" if args.show_progress_bar else None)

    engine_wrapper = HtrEngineWrapper(trainer, evaluator)
    # Set word delimiters to compute the WER
    engine_wrapper.set_word_delimiters([syms[sym] for sym in args.delimiters])

    def ckpt_saver(filename):
        return CheckpointSaver(os.path.join(args.train_path, filename))

    saver = partial(RollingSaver, keep=args.num_rolling_checkpoints)
    tr_saver_best_cer = saver(
        TrainerCheckpointSaver(
            ckpt_saver("engine-wrapper.ckpt.lowest-valid-cer"),
            engine_wrapper,
            gpu=args.gpu,
        )
    )
    tr_saver_best_wer = saver(
        TrainerCheckpointSaver(
            ckpt_saver("engine-wrapper.ckpt.lowest-valid-wer"),
            engine_wrapper,
            gpu=args.gpu,
        )
    )
    mo_saver_best_cer = saver(
        ModelCheckpointSaver(ckpt_saver("model.ckpt.lowest-valid-cer"), model)
    )
    mo_saver_best_wer = saver(
        ModelCheckpointSaver(ckpt_saver("model.ckpt.lowest-valid-wer"), model)
    )

    @action
    def save(saver, epoch):
        saver.save(suffix=epoch)

    # Set hooks
    trainer.add_hook(
        EPOCH_END,
        HookCollection(
            # Save on best CER
            Hook(
                Lowest(engine_wrapper.valid_cer()),
                ActionCollection(
                    Action(save, saver=tr_saver_best_cer),
                    Action(save, saver=mo_saver_best_cer),
                ),
            ),
            # Save on best WER
            Hook(
                Lowest(engine_wrapper.valid_wer()),
                ActionCollection(
                    Action(save, saver=tr_saver_best_wer),
                    Action(save, saver=mo_saver_best_wer),
                ),
            ),
        ),
    )
    if args.save_checkpoint_interval:
        # Save every `save_checkpoint_interval` epochs
        tr_saver = saver(
            TrainerCheckpointSaver(
                ckpt_saver("engine-wrapper.ckpt"), engine_wrapper, gpu=args.gpu
            )
        )
        mo_saver = saver(ModelCheckpointSaver(ckpt_saver("model.ckpt"), model))
        log.get_logger("laia.hooks.conditions.multiple_of").setLevel(log.WARNING)
        trainer.add_hook(
            EPOCH_END,
            Hook(
                MultipleOf(trainer.epochs, args.save_checkpoint_interval),
                ActionCollection(
                    Action(save, saver=tr_saver), Action(save, saver=mo_saver)
                ),
            ),
        )
    if args.max_nondecreasing_epochs:
        # Stop when the validation CER hasn't improved in
        # `max_nondecreasing_epochs` consecutive epochs
        trainer.add_hook(
            EPOCH_END,
            Hook(
                ConsecutiveNonDecreasing(
                    engine_wrapper.valid_cer(), args.max_nondecreasing_epochs
                ),
                trainer.stop,
            ),
        )
    if args.max_epochs:
        # Stop when `max_epochs` has been reached
        trainer.add_hook(
            EPOCH_START, Hook(GEqThan(trainer.epochs, args.max_epochs), trainer.stop)
        )

    # Continue from the given checkpoint, if possible
    TrainerCheckpointLoader(engine_wrapper, gpu=args.gpu).load_by(
        os.path.join(args.train_path, "engine-wrapper.{}".format(args.checkpoint))
    )
    ModelCheckpointLoader(model, gpu=args.gpu).load_by(
        os.path.join(args.train_path, "model.{}".format(args.checkpoint))
    )

    engine_wrapper.run()
