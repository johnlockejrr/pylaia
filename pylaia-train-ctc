#!/usr/bin/env python
from __future__ import absolute_import

from torch.optim import RMSprop

import laia.logging as log
from laia.data import ImageDataLoader, TextImageFromTextTableDataset, FixedSizeSampler
from laia.engine import Trainer, Evaluator, HtrEngineWrapper
from laia.engine.engine import ON_EPOCH_END, ON_EPOCH_START
from laia.engine.feeders import ImageFeeder, ItemFeeder
from laia.hooks import Hook, HookCollection, action
from laia.hooks.conditions import Lowest, MultipleOf, GEqThan
from laia.plugins.arguments import add_argument, args, add_defaults
from laia.plugins.loader import TrainerLoader, ModelLoader, TrainerCheckpointLoader, ModelCheckpointLoader, \
    CheckpointLoader
from laia.plugins.saver import TrainerSaver, ModelCheckpointSaver, TrainerCheckpointSaver, BackupSaver, CheckpointSaver
from laia.utils import SymbolsTable, ImageToTensor, TextToTensor

if __name__ == '__main__':
    add_defaults('batch_size', 'learning_rate', 'momentum', 'gpu',
                 'max_epochs', 'show_progress_bar', 'save_path',
                 'samples_per_epoch', 'iterations_per_update')
    add_argument('syms', help='Symbols table mapping from strings to integers')
    add_argument('img_dir', help='Directory containing word images')
    add_argument('tr_txt_table', help='Character transcriptions of each training image')
    add_argument('va_txt_table', help='Character transcriptions of each validation image')
    args = args()

    syms = SymbolsTable(args.syms)

    model = ModelLoader(args.save_path).load()
    if model is None:
        log.error('Could not find the model. Have you run "pylaia-create-model"?')
        exit(1)
    model = model.cuda(args.gpu - 1) if args.gpu else model.cpu()

    trainer = TrainerLoader(args.save_path).load()
    if trainer is None:
        optimizer = RMSprop(model.parameters(),
                            lr=args.learning_rate,
                            momentum=args.momentum)
        parameters = {
            'model': model,
            'criterion': None,  # Set automatically by HtrEngineWrapper
            'optimizer': optimizer,
            'batch_target_fn': ItemFeeder('txt'),
            'progress_bar': 'Train' if args.show_progress_bar else None}
        trainer = Trainer(**parameters)
        TrainerSaver(args.save_path).save(Trainer, **parameters)

    evaluator = Evaluator(
        model=model,
        batch_target_fn=ItemFeeder('txt'),
        progress_bar='Valid' if args.show_progress_bar else None)

    tr_ds = TextImageFromTextTableDataset(
        args.tr_txt_table, args.img_dir,
        img_transform=ImageToTensor(),
        txt_transform=TextToTensor(syms))

    tr_ds_loader = ImageDataLoader(dataset=tr_ds,
                                   image_channels=1,
                                   batch_size=args.batch_size,
                                   num_workers=8,
                                   shuffle=not bool(args.samples_per_epoch),
                                   sampler=FixedSizeSampler(
                                       tr_ds,
                                       args.samples_per_epoch)
                                   if args.samples_per_epoch else None)

    va_ds = TextImageFromTextTableDataset(
        args.va_txt_table, args.img_dir,
        img_transform=ImageToTensor(),
        txt_transform=TextToTensor(syms))
    va_ds_loader = ImageDataLoader(dataset=va_ds,
                                   image_channels=1,
                                   batch_size=args.batch_size,
                                   num_workers=8)

    # Set all these separately because they might change between executions
    trainer.iterations_per_update = args.iterations_per_update
    trainer.set_data_loader(tr_ds_loader)
    trainer.set_batch_input_fn(ImageFeeder(device=args.gpu,
                                           # keep_padded_tensors=False,
                                           parent_feeder=ItemFeeder('img')))

    evaluator.set_data_loader(va_ds_loader)
    evaluator.set_batch_input_fn(ImageFeeder(device=args.gpu,
                                             parent_feeder=ItemFeeder('img')))

    engine_wrapper = HtrEngineWrapper(trainer, evaluator)


    def valid_cer():
        return engine_wrapper.valid_cer().value


    @action
    def save_ckpt(saver, epoch):
        saver.save(suffix=epoch)


    '''
    @action
    def save_ckpt(saver, filename, epoch):
        saver.filename = filename
        saver.save(suffix=epoch)
        
    
    ckpt_saver = CheckpointSaver(args.save_path)
    tr_ckpt_saver = TrainerCheckpointSaver(trainer, ckpt_saver)
    mo_ckpt_saver = ModelCheckpointSaver(model, ckpt_saver)
    # These cannot be reused because each one keeps a different state
    saver1 = BackupSaver(tr_ckpt_saver)
    saver2 = BackupSaver(tr_ckpt_saver)
    saver3 = BackupSaver(mo_ckpt_saver, keep=3)
    saver4 = BackupSaver(mo_ckpt_saver, keep=3)
    
    
    # Set hooks
    trainer.add_hook(ON_EPOCH_END, HookCollection(
        Hook(Lowest(valid_cer), save_ckpt, saver=saver1, filename='trainer.ckpt-lowest-valid-cer'),
        Hook(Lowest(valid_cer), save_ckpt, saver=saver2, filename='trainer.ckpt'),
        Hook(MultipleOf(trainer.epochs, 5), save_ckpt, saver=saver3, filename='model.ckpt-lowest-valid-cer'),
        Hook(MultipleOf(trainer.epochs, 5), save_ckpt, saver=saver4, filename='model.ckpt')))

    '''

    # TODO: Refactor
    ckpt_saver1 = CheckpointSaver(args.save_path, 'trainer.ckpt-lowest-valid-cer')
    ckpt_saver2 = CheckpointSaver(args.save_path, 'trainer.ckpt')
    ckpt_saver3 = CheckpointSaver(args.save_path, 'model.ckpt-lowest-valid-cer')
    ckpt_saver4 = CheckpointSaver(args.save_path, 'model.ckpt')
    saver1 = BackupSaver(TrainerCheckpointSaver(trainer, ckpt_saver1))
    saver2 = BackupSaver(TrainerCheckpointSaver(trainer, ckpt_saver2))
    saver3 = BackupSaver(ModelCheckpointSaver(model, ckpt_saver3), keep=3)
    saver4 = BackupSaver(ModelCheckpointSaver(model, ckpt_saver4), keep=3)

    # Set hooks
    trainer.add_hook(ON_EPOCH_END, HookCollection(
        Hook(Lowest(valid_cer), save_ckpt, saver=saver1),
        Hook(Lowest(valid_cer), save_ckpt, saver=saver2),
        Hook(MultipleOf(trainer.epochs, 5), save_ckpt, saver=saver3),
        Hook(MultipleOf(trainer.epochs, 5), save_ckpt, saver=saver4)))
    if args.max_epochs and args.max_epochs > 0:
        trainer.add_hook(ON_EPOCH_START,
                         Hook(GEqThan(trainer.epochs, args.max_epochs), trainer.stop))

    # Continue from last checkpoints with lowest valid cer, if possible
    TrainerCheckpointLoader(
        trainer,
        CheckpointLoader(
            args.save_path,
            'trainer.ckpt-lowest-valid-cer')).load_last()
    ModelCheckpointLoader(
        model,
        CheckpointLoader(
            args.save_path,
            'model.ckpt-lowest-valid-cer')).load_last()

    engine_wrapper.run()
